{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc2c62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/andrewmvd/helmet-detection?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391M/391M [00:32<00:00, 12.4MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source import complete.\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "andrewmvd_helmet_detection_path = kagglehub.dataset_download('andrewmvd/helmet-detection')\n",
    "\n",
    "print('Data source import complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a20436c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded to: /Users/kushalgupta/.cache/kagglehub/datasets/andrewmvd/helmet-detection/versions/1\n",
      "Absolute path: /Users/kushalgupta/.cache/kagglehub/datasets/andrewmvd/helmet-detection/versions/1\n",
      "Contents: ['images', 'annotations']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the download path\n",
    "download_path = andrewmvd_helmet_detection_path\n",
    "print(\"Dataset downloaded to:\", download_path)\n",
    "\n",
    "# To get absolute path\n",
    "absolute_path = Path(download_path).resolve()\n",
    "print(\"Absolute path:\", absolute_path)\n",
    "\n",
    "# To check if path exists and show contents\n",
    "if Path(download_path).exists() and Path(download_path).is_dir():\n",
    "    print(\"Contents:\", os.listdir(download_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77890a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "from lxml import etree\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define paths\n",
    "annotations_dir = r\"/Users/kushalgupta/Desktop/DeepLearning/datasets/1/annotations\"\n",
    "images_dir = r\"/Users/kushalgupta/Desktop/DeepLearning/datasets/1/images\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70c10315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: unmatched \"\n"
     ]
    }
   ],
   "source": [
    "# Define classes\n",
    "classes = ['With Helmet', 'Without Helmet']\n",
    "class_dict = {'With Helmet': 0, 'Without Helmet': 1}\n",
    "\n",
    "def parse_annotation(annotation_file):\n",
    "    \"\"\"Parse XML annotations to extract bounding boxes and labels\"\"\"\n",
    "    tree = etree.parse(annotation_file)\n",
    "    root = tree.getroot()\n",
    "    objects = []\n",
    "    for obj in root.findall('object'):\n",
    "        obj_struct = {}\n",
    "        obj_struct['name'] = obj.find('name').text\n",
    "        bbox = obj.find('bndbox')\n",
    "        obj_struct['bbox'] = [int(bbox.find('xmin').text), int(bbox.find('ymin').text),\n",
    "                              int(bbox.find('xmax').text), int(bbox.find('ymax').text)]\n",
    "        objects.append(obj_struct)\n",
    "    return objects\n",
    "\n",
    "def prepare_yolo_dataset(annotations_dir, images_dir, output_dir):\n",
    "    \"\"\"Prepare dataset in YOLO format\"\"\"\n",
    "    # Create directories\n",
    "    os.makedirs(os.path.join(output_dir, 'images', 'train'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'images', 'val'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'labels', 'train'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'labels', 'val'), exist_ok=True)\n",
    "    \n",
    "    annotation_files = [f for f in os.listdir(annotations_dir) if f.endswith('.xml')]\n",
    "    train_files, val_files = train_test_split(annotation_files, test_size=0.2, random_state=42)\n",
    "    \n",
    "    def process_files(files, split):\n",
    "        for annotation_file in files:\n",
    "            # Parse annotation\n",
    "            annotation_path = os.path.join(annotations_dir, annotation_file)\n",
    "            objects = parse_annotation(annotation_path)\n",
    "            \n",
    "            # Image path\n",
    "            image_path = os.path.join(images_dir, annotation_file.replace('.xml', '.png'))\n",
    "            img = cv2.imread(image_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "                \n",
    "            h, w = img.shape[:2]\n",
    "            \n",
    "            # Copy image\n",
    "            img_filename = annotation_file.replace('.xml', '.png')\n",
    "            cv2.imwrite(os.path.join(output_dir, 'images', split, img_filename), img)\n",
    "            \n",
    "            # Create YOLO format labels\n",
    "            label_filename = annotation_file.replace('.xml', '.txt')\n",
    "            label_path = os.path.join(output_dir, 'labels', split, label_filename)\n",
    "            \n",
    "            with open(label_path, 'w') as f:\n",
    "                for obj in objects:\n",
    "                    class_id = class_dict[obj['name']]\n",
    "                    xmin, ymin, xmax, ymax = obj['bbox']\n",
    "                    \n",
    "                    # Convert to YOLO format (normalized)\n",
    "                    x_center = (xmin + xmax) / 2 / w\n",
    "                    y_center = (ymin + ymax) / 2 / h\n",
    "                    width = (xmax - xmin) / w\n",
    "                    height = (ymax - ymin) / h\n",
    "                    \n",
    "                    f.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n",
    "    \n",
    "    process_files(train_files, 'train')\n",
    "    process_files(val_files, 'val')\n",
    "    \n",
    "    # Create dataset.yaml file\n",
    "    yaml_content = f\"\"\"\n",
    "path: {output_dir}\n",
    "train: images/train\n",
    "val: images/val\n",
    "\n",
    "nc: {len(classes)}\n",
    "names: {classes}\n",
    "\"\"\"\n",
    "    \n",
    "    with open(os.path.join(output_dir, 'dataset.yaml'), 'w') as f:\n",
    "        f.write(yaml_content)\n",
    "    \n",
    "    return os.path.join(output_dir, 'dataset.yaml')\n",
    "\n",
    "!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff1dfd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset prepared at: helmet_yolo_dataset\n",
      "WARNING âš ï¸ Download failure, retrying 1/3 https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt... <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "######################################################################## 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.221 ğŸš€ Python-3.11.9 torch-2.9.0 CPU (Apple M2)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=helmet_yolo_dataset/dataset.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/Users/kushalgupta/Desktop/DeepLearning/runs/detect/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,238 parameters, 3,011,222 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 648.9Â±80.4 MB/s, size: 259.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/kushalgupta/Desktop/DeepLearning/helmet_yolo_dataset/labels/train... 611 images, 3 backgrounds, 10 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 611/611 1.8Kit/s 0.3s.1ss\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/Users/kushalgupta/Desktop/DeepLearning/helmet_yolo_dataset/images/train/BikesHelmets103.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [408.72375  81.5019   71.0375   83.0038 ]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/Users/kushalgupta/Desktop/DeepLearning/helmet_yolo_dataset/images/train/BikesHelmets205.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [263.5  43.   85.   74. ]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/Users/kushalgupta/Desktop/DeepLearning/helmet_yolo_dataset/images/train/BikesHelmets326.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [157.   56.5  86.   97. ]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/Users/kushalgupta/Desktop/DeepLearning/helmet_yolo_dataset/images/train/BikesHelmets441.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [310.5  41.5  61.   63. ]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/Users/kushalgupta/Desktop/DeepLearning/helmet_yolo_dataset/images/train/BikesHelmets444.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [299.  48.  78.  80.]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/Users/kushalgupta/Desktop/DeepLearning/helmet_yolo_dataset/images/train/BikesHelmets530.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [313.3525  74.     102.115  116.    ]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/Users/kushalgupta/Desktop/DeepLearning/helmet_yolo_dataset/images/train/BikesHelmets616.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [750.  297.5  68.   91. ]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/Users/kushalgupta/Desktop/DeepLearning/helmet_yolo_dataset/images/train/BikesHelmets671.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [863. 251.  52.  52.]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/Users/kushalgupta/Desktop/DeepLearning/helmet_yolo_dataset/images/train/BikesHelmets706.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [379.5  82.5  61.   65. ]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/Users/kushalgupta/Desktop/DeepLearning/helmet_yolo_dataset/images/train/BikesHelmets80.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [424.5      45.29778  71.       82.63111]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/kushalgupta/Desktop/DeepLearning/helmet_yolo_dataset/labels/train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 672.6Â±157.6 MB/s, size: 235.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/kushalgupta/Desktop/DeepLearning/helmet_yolo_dataset/labels/val... 153 images, 0 backgrounds, 5 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 153/153 1.9Kit/s 0.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/Users/kushalgupta/Desktop/DeepLearning/helmet_yolo_dataset/images/val/BikesHelmets140.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [547.685  84.5   132.165 151.   ]\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/Users/kushalgupta/Desktop/DeepLearning/helmet_yolo_dataset/images/val/BikesHelmets279.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [194.5  56.  103.  100. ]\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/Users/kushalgupta/Desktop/DeepLearning/helmet_yolo_dataset/images/val/BikesHelmets343.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [682.27 150.5  118.22 107.  ]\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/Users/kushalgupta/Desktop/DeepLearning/helmet_yolo_dataset/images/val/BikesHelmets75.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [471.  290.5  82.  109.  761.  291.5  84.   97.  466.5 297.  109.  118. ]\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/Users/kushalgupta/Desktop/DeepLearning/helmet_yolo_dataset/images/val/BikesHelmets764.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [219.  91.  80.  82.]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/kushalgupta/Desktop/DeepLearning/helmet_yolo_dataset/labels/val.cache\n",
      "Plotting labels to /Users/kushalgupta/Desktop/DeepLearning/runs/detect/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/Users/kushalgupta/Desktop/DeepLearning/runs/detect/train\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/50         0G      1.456      2.754       1.21         36        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 3:032.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 16.7s4.7s\n",
      "                   all        148        293     0.0061      0.916      0.284      0.161\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/50         0G      1.424      1.842      1.173         19        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 3:012.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 16.7s4.7s\n",
      "                   all        148        293      0.821      0.142      0.474      0.242\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/50         0G      1.434      1.687      1.195         25        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 3:012.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 16.5s4.6s\n",
      "                   all        148        293      0.633      0.542      0.567      0.299\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/50         0G      1.431       1.57        1.2         39        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 3:012.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 16.3s4.5s\n",
      "                   all        148        293      0.594      0.563      0.574      0.329\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/50         0G      1.427      1.503      1.194         31        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 2:602.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 16.0s4.5s\n",
      "                   all        148        293      0.619      0.707      0.662      0.356\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/50         0G      1.372      1.317      1.172         30        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.1it/s 7:176.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 16.4s4.6s\n",
      "                   all        148        293       0.67      0.662        0.7      0.382\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/50         0G      1.366      1.223      1.173         28        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 3:012.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 16.0s4.5s\n",
      "                   all        148        293      0.556      0.686      0.634      0.362\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/50         0G      1.348      1.247      1.156         27        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 3:012.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 16.2s4.5s\n",
      "                   all        148        293      0.692      0.683      0.698      0.385\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/50         0G      1.343      1.172      1.156         31        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 3:012.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 16.0s4.5s\n",
      "                   all        148        293      0.631      0.603      0.597      0.307\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/50         0G      1.327      1.134      1.149         31        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 3:043.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 18.4s5.2ss\n",
      "                   all        148        293        0.7      0.769      0.764      0.427\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      11/50         0G      1.359      1.073      1.151         22        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 3:093.0ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 17.7s4.9s\n",
      "                   all        148        293       0.59      0.732       0.69      0.394\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      12/50         0G      1.332      1.049      1.137         23        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 3:092.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 16.7s4.7s\n",
      "                   all        148        293      0.665      0.721      0.744       0.42\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      13/50         0G      1.279      1.005      1.116         25        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 3:093.0ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 17.2s4.8s\n",
      "                   all        148        293      0.739       0.68      0.729      0.398\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      14/50         0G      1.303      1.001      1.133         22        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 3:103.0ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 17.0s4.8s\n",
      "                   all        148        293      0.669      0.782      0.765       0.47\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      15/50         0G      1.301     0.9625      1.138         39        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 3:042.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 16.1s4.5s\n",
      "                   all        148        293      0.699      0.743       0.75      0.428\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      16/50         0G      1.253     0.9224      1.115         22        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 2:593.0ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 16.0s4.5s\n",
      "                   all        148        293      0.756      0.733      0.799      0.469\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      17/50         0G      1.267     0.9013      1.114         49        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 3:043.0ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 16.7s4.7s\n",
      "                   all        148        293      0.733      0.769      0.795       0.45\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      18/50         0G      1.267     0.9043      1.123         23        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 2:592.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 16.1s4.5s\n",
      "                   all        148        293      0.647      0.784      0.728      0.434\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      19/50         0G      1.245      0.905      1.104         38        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 3:022.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 16.6s4.6s\n",
      "                   all        148        293      0.724       0.69      0.754       0.44\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      20/50         0G      1.253     0.8791      1.118         35        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 3:032.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 16.6s4.6s\n",
      "                   all        148        293       0.66       0.72      0.741      0.432\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      21/50         0G      1.232     0.8753      1.096         20        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 3:042.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 16.5s4.6s\n",
      "                   all        148        293      0.639      0.769      0.771       0.45\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      22/50         0G      1.219     0.8024      1.087         23        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 3:012.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 16.5s4.6s\n",
      "                   all        148        293      0.733      0.785      0.792      0.476\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      23/50         0G      1.194       0.81      1.073         30        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 3:022.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 16.5s4.6s\n",
      "                   all        148        293      0.685      0.766      0.781      0.445\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      24/50         0G      1.203     0.8297      1.093         29        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 3:022.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 16.6s4.6s\n",
      "                   all        148        293      0.796      0.648      0.746      0.424\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      25/50         0G      1.196     0.7943      1.079         36        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 3:183.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 18.0s5.1ss\n",
      "                   all        148        293      0.772      0.772      0.814      0.489\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      26/50         0G      1.181     0.7786      1.067         34        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 3:183.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 17.9s5.0ss\n",
      "                   all        148        293      0.777       0.77      0.837      0.484\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      27/50         0G      1.184     0.7834      1.089         19        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 3:243.3ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 19.7s5.5ss\n",
      "                   all        148        293      0.744      0.744      0.788      0.463\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      28/50         0G      1.178     0.7689      1.074         30        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 3:263.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 18.5s5.2ss\n",
      "                   all        148        293      0.793      0.776      0.811      0.473\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      29/50         0G      1.161     0.7617      1.057         35        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 3:183.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 18.1s5.1ss\n",
      "                   all        148        293      0.743      0.718       0.78      0.466\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      30/50         0G      1.144     0.7441      1.061         27        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 3:193.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 18.2s5.1ss\n",
      "                   all        148        293      0.793      0.709      0.801      0.462\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      31/50         0G      1.164     0.7228      1.063         34        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 3:213.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 17.7s5.0s\n",
      "                   all        148        293      0.781      0.737        0.8      0.474\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      32/50         0G      1.146     0.7044      1.062         31        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 3:042.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 16.1s4.5s\n",
      "                   all        148        293      0.719      0.796      0.819      0.479\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      33/50         0G      1.142     0.6747      1.055         21        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 3:103.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 17.2s4.8s\n",
      "                   all        148        293      0.716       0.77      0.802      0.475\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      34/50         0G      1.118     0.6939      1.055         29        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 3:092.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 16.8s4.7s\n",
      "                   all        148        293      0.764      0.785      0.799      0.469\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      35/50         0G      1.104     0.6867      1.033         39        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 38/38 0.2it/s 3:052.9ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 16.6s4.6s\n",
      "                   all        148        293      0.753      0.775      0.828      0.486\n",
      "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 10 epochs. Best results observed at epoch 25, best model saved as best.pt.\n",
      "To update EarlyStopping(patience=10) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n",
      "\n",
      "35 epochs completed in 2.054 hours.\n",
      "Optimizer stripped from /Users/kushalgupta/Desktop/DeepLearning/runs/detect/train/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from /Users/kushalgupta/Desktop/DeepLearning/runs/detect/train/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating /Users/kushalgupta/Desktop/DeepLearning/runs/detect/train/weights/best.pt...\n",
      "Ultralytics 8.3.221 ğŸš€ Python-3.11.9 torch-2.9.0 CPU (Apple M2)\n",
      "Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.3it/s 16.0s4.5s\n",
      "                   all        148        293      0.772      0.772      0.814      0.488\n",
      "           With Helmet        107        193       0.79      0.845      0.878      0.587\n",
      "        Without Helmet         48        100      0.755        0.7       0.75      0.389\n",
      "Speed: 0.7ms preprocess, 100.8ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1m/Users/kushalgupta/Desktop/DeepLearning/runs/detect/train\u001b[0m\n",
      "Ultralytics 8.3.221 ğŸš€ Python-3.11.9 torch-2.9.0 CPU (Apple M2)\n",
      "Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 5792.5Â±1029.7 MB/s, size: 244.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/kushalgupta/Desktop/DeepLearning/helmet_yolo_dataset/labels/val.cache... 153 images, 0 backgrounds, 5 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 153/153 690.0Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/Users/kushalgupta/Desktop/DeepLearning/helmet_yolo_dataset/images/val/BikesHelmets140.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [547.685  84.5   132.165 151.   ]\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/Users/kushalgupta/Desktop/DeepLearning/helmet_yolo_dataset/images/val/BikesHelmets279.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [194.5  56.  103.  100. ]\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/Users/kushalgupta/Desktop/DeepLearning/helmet_yolo_dataset/images/val/BikesHelmets343.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [682.27 150.5  118.22 107.  ]\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/Users/kushalgupta/Desktop/DeepLearning/helmet_yolo_dataset/images/val/BikesHelmets75.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [471.  290.5  82.  109.  761.  291.5  84.   97.  466.5 297.  109.  118. ]\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/Users/kushalgupta/Desktop/DeepLearning/helmet_yolo_dataset/images/val/BikesHelmets764.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [219.  91.  80.  82.]\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 10/10 0.6it/s 16.6s.1s\n",
      "                   all        148        293      0.767      0.765      0.815      0.489\n",
      "           With Helmet        107        193      0.797      0.839      0.881      0.588\n",
      "        Without Helmet         48        100      0.737       0.69      0.748      0.391\n",
      "Speed: 0.5ms preprocess, 105.2ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1m/Users/kushalgupta/Desktop/DeepLearning/runs/detect/val\u001b[0m\n",
      "YOLOv8 Validation Metrics:\n",
      "mAP50: 0.8146\n",
      "mAP50-95: 0.4895\n",
      "Precision: 0.7671\n",
      "Recall: 0.7647\n",
      "\n",
      "image 1/1 /Users/kushalgupta/Desktop/DeepLearning/helmet_yolo_dataset/images/val/BikesHelmets573.png: 448x640 2 With Helmets, 1 Without Helmet, 43.1ms\n",
      "Speed: 1.0ms preprocess, 43.1ms inference, 0.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "image 1/1 /Users/kushalgupta/Desktop/DeepLearning/helmet_yolo_dataset/images/val/BikesHelmets17.png: 384x640 2 With Helmets, 36.6ms\n",
      "Speed: 0.9ms preprocess, 36.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /Users/kushalgupta/Desktop/DeepLearning/helmet_yolo_dataset/images/val/BikesHelmets361.png: 448x640 1 With Helmet, 44.9ms\n",
      "Speed: 1.0ms preprocess, 44.9ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "image 1/1 /Users/kushalgupta/Desktop/DeepLearning/helmet_yolo_dataset/images/val/BikesHelmets360.png: 448x640 3 With Helmets, 43.3ms\n",
      "Speed: 0.9ms preprocess, 43.3ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "image 1/1 /Users/kushalgupta/Desktop/DeepLearning/helmet_yolo_dataset/images/val/BikesHelmets176.png: 448x640 2 Without Helmets, 43.2ms\n",
      "Speed: 1.0ms preprocess, 43.2ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x400 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv8 Training Completed\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset\n",
    "output_dir = 'helmet_yolo_dataset'\n",
    "yaml_path = prepare_yolo_dataset(annotations_dir, images_dir, output_dir)\n",
    "print(f\"Dataset prepared at: {output_dir}\")\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')  # Using nano version for faster training\n",
    "\n",
    "# Train the model\n",
    "results = model.train(\n",
    "    data=yaml_path,\n",
    "    epochs=50,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    device=str(device),\n",
    "    patience=10,\n",
    "    save=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Load the best model\n",
    "best_model_path = 'runs/detect/train/weights/best.pt'\n",
    "model = YOLO(best_model_path)\n",
    "\n",
    "# Evaluate the model\n",
    "metrics = model.val()\n",
    "print(\"YOLOv8 Validation Metrics:\")\n",
    "print(f\"mAP50: {metrics.box.map50:.4f}\")\n",
    "print(f\"mAP50-95: {metrics.box.map:.4f}\")\n",
    "print(f\"Precision: {metrics.box.mp:.4f}\")\n",
    "print(f\"Recall: {metrics.box.mr:.4f}\")\n",
    "\n",
    "# Test the model on sample images\n",
    "def test_yolo_model(model, test_images_dir, num_images=5):\n",
    "    \"\"\"Test YOLOv8 model on sample images\"\"\"\n",
    "    image_files = [f for f in os.listdir(test_images_dir) if f.endswith('.png')][:num_images]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(20, 4))\n",
    "    if num_images == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, img_file in enumerate(image_files):\n",
    "        img_path = os.path.join(test_images_dir, img_file)\n",
    "        \n",
    "        # Perform detection\n",
    "        results = model(img_path)\n",
    "        \n",
    "        # Plot results\n",
    "        result_img = results[0].plot()\n",
    "        result_img_rgb = cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        axes[i].imshow(result_img_rgb)\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f'Detection: {img_file}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Test on validation images\n",
    "test_images_dir = os.path.join(output_dir, 'images', 'val')\n",
    "test_yolo_model(model, test_images_dir)\n",
    "\n",
    "# Save YOLO model performance metrics\n",
    "yolo_metrics = {\n",
    "    'map50': metrics.box.map50,\n",
    "    'map50_95': metrics.box.map,\n",
    "    'precision': metrics.box.mp,\n",
    "    'recall': metrics.box.mr\n",
    "}\n",
    "\n",
    "print(\"YOLOv8 Training Completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
